{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Alignment (Feature Based) using OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many Computer Vision applications, we often need to identify interesting stable points in an image. \n",
    "These points are called keypoints or feature points. \n",
    "There are several keypoint detectors implemented in OpenCV ( e.g. SIFT, SURF, and ORB).\n",
    "\n",
    "ORB stands for Oriented FAST and Rotated BRIEF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A feature point detector has two parts :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locator: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This identifies points on the image that are stable under image transformations like translation, scale and rotation.\n",
    "The locator finds the x, y coordinates of such points. The locator used by the ORB detector is called FAST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The locator in the above step only tells us where the interesting points are.\n",
    "The second part of the feature detector is the descriptor which encodes the appearance of the point \n",
    "Descriptor can tell one feature point from the other. \n",
    "Ideally, the same physical point in two images should have the same descriptor. \n",
    "ORB uses a modified version of the feature descriptor called BRISK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Homography that relates the two images can be calculated only if we know corresponding features in the two images. \n",
    "So a matching algorithm is used to find which features in one image match features in the other image. \n",
    "The descriptor of every feature in one image is compared to the descriptor of every feature in the second image to find good matches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homography:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two images of a scene are related by a homography under two conditions.\n",
    "\n",
    "1. The two images are that of a plane (e.g. sheet of paper, credit card etc.).\n",
    "2. The two images were acquired by rotating the camera about its optical axis. We take such images while generating panoramas.\n",
    "\n",
    "A homography is nothing but a 3×3 matrix ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](homography.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let (x1,y1) be a point in the first image and (x2,y2) be the coordinates of the same physical point in the second image.\n",
    "Then, the Homography H relates them in the following way :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](homography2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we knew the homography, we could apply it to all the pixels of one image to obtain a warped image that is aligned with the second image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading reference image :  inter_doc.png\n",
      "Reading image to align :  inter_doc_totrot.jpg\n",
      "Aligning images ...\n",
      "Saving aligned image :  aligned.jpg\n",
      "Estimated homography : \n",
      " [[-1.00348921e+00  2.12942270e-03  7.55446845e+02]\n",
      " [-7.18917806e-04 -1.00032145e+00  9.71910836e+02]\n",
      " [ 1.32739469e-06  3.96730867e-06  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MAX_FEATURES = 500\n",
    "GOOD_MATCH_PERCENT = 0.15\n",
    " \n",
    " \n",
    "def alignImages(im1, im2):\n",
    " \n",
    "  # Convert images to grayscale\n",
    "  im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "  im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "   \n",
    "  # Detect ORB features and compute descriptors.\n",
    "  orb = cv2.ORB_create(MAX_FEATURES)\n",
    "  keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, None)\n",
    "  keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, None)\n",
    "   \n",
    "  # Match features.\n",
    "  matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "  matches = matcher.match(descriptors1, descriptors2, None)\n",
    "   \n",
    "  # Sort matches by score\n",
    "  matches.sort(key=lambda x: x.distance, reverse=False)\n",
    " \n",
    "  # Remove not so good matches\n",
    "  numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "  matches = matches[:numGoodMatches]\n",
    " \n",
    "  # Draw top matches\n",
    "  imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "  cv2.imwrite(\"matches.jpg\", imMatches)\n",
    "   \n",
    "  # Extract location of good matches\n",
    "  points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "  points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    " \n",
    "  for i, match in enumerate(matches):\n",
    "    points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "    points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "   \n",
    "  # Find homography\n",
    "  h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    " \n",
    "  # Use homography\n",
    "  height, width, channels = im2.shape\n",
    "  im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "   \n",
    "  return im1Reg, h\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "   \n",
    "  # Read reference image\n",
    "  refFilename = \"inter_doc.png\"\n",
    "  print(\"Reading reference image : \", refFilename)\n",
    "    \n",
    "  #Using the inbuilt cv2 function to read the image.  \n",
    "  imReference = cv2.imread(refFilename, cv2.IMREAD_COLOR)\n",
    " \n",
    "  # Read image to be aligned\n",
    "  imFilename = \"inter_doc_totrot.jpg\"\n",
    "  print(\"Reading image to align : \", imFilename);\n",
    "    \n",
    "  #Using the inbuilt cv2 function to read the image.  \n",
    "  im = cv2.imread(imFilename, cv2.IMREAD_COLOR)\n",
    "   \n",
    "  print(\"Aligning images ...\")\n",
    "  # Registered image will be resotred in imReg. \n",
    "  # The estimated homography will be stored in h. \n",
    "  imReg, h = alignImages(im, imReference)\n",
    "   \n",
    "  # Write aligned image to disk. \n",
    "  outFilename = \"aligned.jpg\"\n",
    "  print(\"Saving aligned image : \", outFilename);\n",
    "\n",
    "  #Using cv2 inbuilt function to save the output image  \n",
    "  cv2.imwrite(outFilename, imReg)\n",
    " \n",
    "  # Print estimated homography\n",
    "  print(\"Estimated homography : \\n\",  h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for Feature Based Image Alignment :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Images :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read the reference image (or the template image) and the image we want to align to this template in Lines 56-65 in the Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then detect ORB features in the two images. Although we need only 4 features to compute the homography, typically hundreds of features are detected in the two images. We control the number of features using the parameter MAX_FEATURES in the Python Code.Lines 16-19 in the Python code detect features and compute the descriptors using detectAndCompute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lines 21-34 in Python we find the matching features in the two images, sort them by goodness of match and keep only a small percentage of original matches. We finally display the good matches on the images and write the file to disk for visual inspection. We use the hamming distance as a measure of similarity between two feature descriptors. The matched features are shown in the figure below by drawing a line connecting them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Homography:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A homography can be computed when we have 4 or more corresponding points in two images. Automatic feature matching explained in the previous section does not always produce 100% accurate matches. It is not uncommon for 20-30% of the matches to be incorrect. Fortunately, the findHomography method utilizes a robust estimation technique called Random Sample Consensus (RANSAC) which produces the right result even in the presence of large number of bad matches. Lines 36-45 in Python accomplish this in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warping image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once an accurate homography has been calculated, the transformation can be applied to all pixels in one image to map it to the other image. This is done using the warpPerspective function in OpenCV. This is accomplished in Line 49 in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Canny Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('inter_doc.png',0)\n",
    "\n",
    "canny = cv2.Canny(img, 50, 120)\n",
    "\n",
    "\n",
    "# scale_percent = 40 # percent of original size\n",
    "# width = int(canny.shape[1] * scale_percent / 100)\n",
    "# height = int(canny.shape[0] * scale_percent / 100)\n",
    "# dim = (width, height)\n",
    "# resize image\n",
    "# resized = cv2.resize(canny, dim, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "_,mask = cv2.threshold(canny,70,255,cv2.THRESH_BINARY_INV)\n",
    "\n",
    "kernel = np.ones((1,1),np.uint8)\n",
    "closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "erosion = cv2.erode(mask,kernel,iterations = 1)\n",
    "opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "cv2.imshow('Canny', closing)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur = cv2.GaussianBlur(img,(1,1),0)\n",
    "_,image = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "cv2.imshow('OTSU',image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Transformations :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphological transformations are some simple operations based on the image shape. It is normally performed on binary images. It needs two inputs, one is our original image, second one is called structuring element or kernel which decides the nature of operation. Two basic morphological operators are Erosion and Dilation. Then its variant forms like Opening, Closing, Gradient etc also comes into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#Reading the Image and converting into gray_scale using the 2nd parameter of imread()\n",
    "img = cv2.imread('inter_doc.png',0)\n",
    "kernel = np.ones((1,1),np.uint8)\n",
    "\n",
    "#Applying Dialation and Erosion to remove noise\n",
    "closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "#Displaying the output until a key is pressed\n",
    "cv2.imshow('Closing Morphology : ', closing)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "#Closing all the open windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Closing is reverse of Opening, Dilation followed by Erosion. It is useful in closing small holes inside the foreground objects, or small black points on the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](closing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THRESHOLDING :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are Two types of Thresholding used in opencv :-\n",
    "1. Simple Thresholding\n",
    "2. Adaptive Thresholding\n",
    "\n",
    "Simple Thresholding - If pixel value is greater than a threshold value, it is assigned one value (may be white), else it is assigned another value (may be black). The function used is cv2.threshold().\n",
    "\n",
    "Adaptive Thresholding - In simple thresholding we used a global value as threshold value. But it may not be good in all the conditions where image has different lighting conditions in different areas. In that case, we go for adaptive thresholding. In this, the algorithm calculate the threshold for a small regions of the image. So we get different thresholds for different regions of the same image and it gives us better results for images with varying illumination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thresholding has a huge influence on the performance of the various processes, because these are based on the data output coming from the thresholding operation. Improper thresholding can cause blotches, streaks or erasures on the document as well as confounding segmentation and recognition tasks. If someone performs optical character recognition (OCR), merges, fractures and other deformations on character shapes are the main reason of performance loss.\n",
    "\n",
    "Here Thresholding is applied in :-\n",
    "\n",
    "• Document image analysis (extracting characters, graphical content, musical scores, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "blur = cv2.GaussianBlur(img,(1,1),0)\n",
    "\n",
    "_,image = cv2.threshold(blur,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "#_,image = cv2.threshold(blur,200,255,cv2.THRESH_BINARY)\n",
    "#image = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 31, 2)\n",
    "\n",
    "cv2.imshow('original',img)\n",
    "cv2.imshow('OTSU',image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
