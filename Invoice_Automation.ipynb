{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required modules and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a minimalist, highly modular neural networks library written in Python and capable on running on top of either TensorFlow or Theano.\n",
    "\n",
    "We have built this notebook using tensorflow as the backend for keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "#import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 64, 64\n",
    "\n",
    "train_data_dir = 'C:\\\\Users\\\\dell\\\\CitiProject\\\\images\\\\train'\n",
    "validation_data_dir = 'C:\\\\Users\\\\dell\\\\CitiProject\\\\images\\\\validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch_size is the number of samples per gradient update. If unspecified, batch_size will default to 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ImageDataGenerator class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate batches of tensor image data with real-time data augmentation(application of random transformation to the train set to artificially enhance the dataset with new unseen images.\n",
    "This will hopefully reduce overfitting and will allow better generalization capability for our network.). The data will be looped over (in batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 images belonging to 2 classes.\n",
      "Found 7 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=2,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=2,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Conv Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture definition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two main types of models available in Keras: the Sequential model, and the Model class used with the functional API.\n",
    "Here we have used Sequential model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential model is a linear stack of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 convolution layers with 2*2 max-pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max-pooling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a technique used to reduce the dimensions of an image by taking the maximum pixel value of a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3...)`\n",
      "  \n",
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "  \n",
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of epochs are the number of iterations on a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 30\n",
    "nb_train_samples = 7\n",
    "nb_validation_samples = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_generator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains the model on data generated batch-by-batch by a Python generator (or an instance of Sequence).\n",
    "The generator is run in parallel to the model, for efficiency. For instance, this allows you to do real-time data augmentation on images on CPU in parallel to training your model on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., steps_per_epoch=3, epochs=30, validation_steps=7)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3/3 [==============================] - 2s 532ms/step - loss: 0.6754 - acc: 0.6667 - val_loss: 0.6689 - val_acc: 0.6923\n",
      "Epoch 2/30\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5853 - acc: 0.7000 - val_loss: 0.6340 - val_acc: 0.7500\n",
      "Epoch 3/30\n",
      "3/3 [==============================] - 0s 24ms/step - loss: 0.5446 - acc: 0.8250 - val_loss: 0.6780 - val_acc: 0.7500\n",
      "Epoch 4/30\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.9534 - acc: 0.6500 - val_loss: 0.8128 - val_acc: 0.6667\n",
      "Epoch 5/30\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.9163 - acc: 0.6667 - val_loss: 0.6442 - val_acc: 0.6923\n",
      "Epoch 6/30\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6059 - acc: 0.3500 - val_loss: 0.6181 - val_acc: 0.7500\n",
      "Epoch 7/30\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.7005 - acc: 0.3500 - val_loss: 0.6124 - val_acc: 0.7500\n",
      "Epoch 8/30\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.6417 - acc: 0.8250 - val_loss: 0.6462 - val_acc: 0.6667\n",
      "Epoch 9/30\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.6545 - acc: 0.5000 - val_loss: 0.5923 - val_acc: 0.7692\n",
      "Epoch 10/30\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.6689 - acc: 0.5250 - val_loss: 0.6226 - val_acc: 0.6667\n",
      "Epoch 11/30\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.6131 - acc: 0.7000 - val_loss: 0.6030 - val_acc: 0.6667\n",
      "Epoch 12/30\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.8068 - acc: 0.5250 - val_loss: 0.5543 - val_acc: 0.7500\n",
      "Epoch 13/30\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5976 - acc: 0.6667 - val_loss: 0.5886 - val_acc: 0.6923\n",
      "Epoch 14/30\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.5945 - acc: 0.8250 - val_loss: 0.5504 - val_acc: 0.6667\n",
      "Epoch 15/30\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.3831 - acc: 1.0000 - val_loss: 0.4683 - val_acc: 0.7500\n",
      "Epoch 16/30\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6097 - acc: 0.6500 - val_loss: 0.4381 - val_acc: 0.7500\n",
      "Epoch 17/30\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.3113 - acc: 0.8333 - val_loss: 0.4921 - val_acc: 0.7692\n",
      "Epoch 18/30\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 1.0343 - acc: 0.5250 - val_loss: 0.4537 - val_acc: 1.0000\n",
      "Epoch 19/30\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.4708 - acc: 1.0000 - val_loss: 0.5904 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.6424 - acc: 0.5250 - val_loss: 0.5521 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "3/3 [==============================] - 0s 25ms/step - loss: 0.5596 - acc: 0.8333 - val_loss: 0.5176 - val_acc: 0.6923\n",
      "Epoch 22/30\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.5400 - acc: 0.8250 - val_loss: 0.5927 - val_acc: 0.6667\n",
      "Epoch 23/30\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 0.6005 - acc: 0.6500 - val_loss: 0.4293 - val_acc: 0.7500\n",
      "Epoch 24/30\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3650 - acc: 0.8250 - val_loss: 0.3884 - val_acc: 0.7500\n",
      "Epoch 25/30\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.3741 - acc: 0.8333 - val_loss: 0.3655 - val_acc: 0.6923\n",
      "Epoch 26/30\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.4021 - acc: 0.7000 - val_loss: 0.2514 - val_acc: 0.7500\n",
      "Epoch 27/30\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3900 - acc: 0.5250 - val_loss: 0.2083 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.1456 - acc: 1.0000 - val_loss: 0.2085 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "3/3 [==============================] - 0s 22ms/step - loss: 0.2288 - acc: 0.8333 - val_loss: 0.1577 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.1091 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x288db03c1d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        samples_per_epoch=nb_train_samples,\n",
    "        nb_epoch=nb_epoch,\n",
    "        validation_data=validation_generator,\n",
    "        nb_val_samples=nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:\\\\Users\\\\dell\\\\CitiProject\\\\model\\\\akshika.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing loss and accuracy :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "returns the loss value and accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate_generator:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluates the model on a data generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10386092440846066, 1.0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(validation_generator, nb_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import glob\n",
    "from PIL import Image\n",
    "from pytesseract import image_to_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Alignment (Feature Based) using OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many Computer Vision applications, we often need to identify interesting stable points in an image. \n",
    "These points are called keypoints or feature points. \n",
    "There are several keypoint detectors implemented in OpenCV ( e.g. SIFT, SURF, and ORB).\n",
    "\n",
    "ORB stands for Oriented FAST and Rotated BRIEF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A feature point detector has two parts :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locator: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This identifies points on the image that are stable under image transformations like translation, scale and rotation.\n",
    "The locator finds the x, y coordinates of such points. The locator used by the ORB detector is called FAST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The locator in the above step only tells us where the interesting points are.\n",
    "The second part of the feature detector is the descriptor which encodes the appearance of the point \n",
    "Descriptor can tell one feature point from the other. \n",
    "Ideally, the same physical point in two images should have the same descriptor. \n",
    "ORB uses a modified version of the feature descriptor called BRISK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Homography that relates the two images can be calculated only if we know corresponding features in the two images. \n",
    "So a matching algorithm is used to find which features in one image match features in the other image. \n",
    "The descriptor of every feature in one image is compared to the descriptor of every feature in the second image to find good matches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homography:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two images of a scene are related by a homography under two conditions.\n",
    "\n",
    "1. The two images are that of a plane (e.g. sheet of paper, credit card etc.).\n",
    "2. The two images were acquired by rotating the camera about its optical axis. We take such images while generating panoramas.\n",
    "\n",
    "A homography is nothing but a 3×3 matrix ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](homography.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let (x1,y1) be a point in the first image and (x2,y2) be the coordinates of the same physical point in the second image.\n",
    "Then, the Homography H relates them in the following way :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](homography2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we knew the homography, we could apply it to all the pixels of one image to obtain a warped image that is aligned with the second image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignImages(im1, im2):\n",
    "    \n",
    "    MAX_FEATURES = 500\n",
    "    GOOD_MATCH_PERCENT = 0.15\n",
    " \n",
    "    # Convert images to grayscale\n",
    "    im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "    im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "   \n",
    "    # Detect ORB features and compute descriptors.\n",
    "    orb = cv2.ORB_create(MAX_FEATURES)\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, None)\n",
    "   \n",
    "    # Match features.\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "    matches = matcher.match(descriptors1, descriptors2, None)\n",
    "   \n",
    "    # Sort matches by score\n",
    "    matches.sort(key=lambda x: x.distance, reverse=False)\n",
    " \n",
    "    # Remove not so good matches\n",
    "    numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "    matches = matches[:numGoodMatches]\n",
    " \n",
    "    # Draw top matches\n",
    "    imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "    #cv2.imwrite(\"matches.jpg\", imMatches)\n",
    "   \n",
    "    # Extract location of good matches\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    " \n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "   \n",
    "    # Find homography\n",
    "    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    " \n",
    "    # Use homography\n",
    "    height, width, channels = im2.shape\n",
    "    im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "   \n",
    "    return im1Reg, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for Feature Based Image Alignment :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Images :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read the reference image (or the template image) and the image we want to align to this template in Lines 56-65 in the Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then detect ORB features in the two images. Although we need only 4 features to compute the homography, typically hundreds of features are detected in the two images. We control the number of features using the parameter MAX_FEATURES in the Python Code.Lines 16-19 in the Python code detect features and compute the descriptors using detectAndCompute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lines 21-34 in Python we find the matching features in the two images, sort them by goodness of match and keep only a small percentage of original matches. We finally display the good matches on the images and write the file to disk for visual inspection. We use the hamming distance as a measure of similarity between two feature descriptors. The matched features are shown in the figure below by drawing a line connecting them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Homography:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A homography can be computed when we have 4 or more corresponding points in two images. Automatic feature matching explained in the previous section does not always produce 100% accurate matches. It is not uncommon for 20-30% of the matches to be incorrect. Fortunately, the findHomography method utilizes a robust estimation technique called Random Sample Consensus (RANSAC) which produces the right result even in the presence of large number of bad matches. Lines 36-45 in Python accomplish this in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warping image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once an accurate homography has been calculated, the transformation can be applied to all pixels in one image to map it to the other image. This is done using the warpPerspective function in OpenCV. This is accomplished in Line 49 in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_for_alignment(test_image_path,reference_image_path):\n",
    "    # Read reference image\n",
    "    refFilename = reference_image_path\n",
    "    \n",
    "    #Using the inbuilt cv2 function to read the image.  \n",
    "    imReference = cv2.imread(refFilename, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    # Read image to be aligned\n",
    "    imFilename = test_image_path\n",
    "    \n",
    "    #Using the inbuilt cv2 function to read the image.  \n",
    "    im = cv2.imread(imFilename, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    # Registered image will be resotred in imReg. \n",
    "    # The estimated homography will be stored in h. \n",
    "    imReg, h = alignImages(im, imReference)\n",
    "    \n",
    "    # Write aligned image to disk. \n",
    "    outFilename = test_image_path\n",
    "    \n",
    "    #Using cv2 inbuilt function to save the output image  \n",
    "    cv2.imwrite(outFilename, imReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Transformations :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphological transformations are some simple operations based on the image shape. It is normally performed on binary images. It needs two inputs, one is our original image, second one is called structuring element or kernel which decides the nature of operation. Two basic morphological operators are Erosion and Dilation. Then its variant forms like Opening, Closing, Gradient etc also comes into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphological_transformation(path):\n",
    "    #Reading the Image and converting into gray_scale using the 2nd parameter of imread()\n",
    "    image = cv2.imread(path,0)\n",
    "    kernel = np.ones((1,1),np.uint8)\n",
    "\n",
    "    #Applying Dialation and Erosion to remove noise\n",
    "    image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "    cv2.imwrite(path,image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    image_list = glob.glob(folder+'/*.jpg')\n",
    "    #add_img = glob.glob(folder+'/*.jpeg')\n",
    "    #image_list.append(img for img in add_img)\n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cit,\n",
      "ES\n",
      "\n",
      "4109 4001 2345 6789\n",
      "\n",
      "art VEN\n",
      "\n",
      "Be tea\n",
      "\n",
      " \n",
      "\n",
      "Peni e\n",
      "\n",
      "\n",
      "**********************************************\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "**********************************************\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "**********************************************\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "**********************************************\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "**********************************************\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "**********************************************\n",
      "0\n",
      "\n",
      "\n",
      "\n",
      "**********************************************\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    images_path = load_images_from_folder(r'C:\\Users\\dell\\CitiProject\\images\\test')\n",
    "    model = load_model('C:/Users/dell/CitiProject/model/akshika.h5')\n",
    "    for path in images_path:\n",
    "        morphological_transformation(path)\n",
    "        \n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img,(64,64))\n",
    "        img = np.reshape(img,[1,64,64,3])\n",
    "        classes = model.predict_classes(img)\n",
    "        \n",
    "        image_type = classes[0][0]\n",
    "        print(image_type)\n",
    "        reference_path = r'C:\\Users\\dell\\CitiProject\\referencefol\\1.jpg'\n",
    "        \n",
    "        if image_type==1:\n",
    "            reference_path = r'C:\\Users\\dell\\CitiProject\\referencefol\\1.jpg'\n",
    "        else:\n",
    "            reference_path = r'C:\\Users\\dell\\CitiProject\\referencefol\\0.jpg'\n",
    "        \n",
    "        #path_for_alignment(path , reference_path)\n",
    "        \n",
    "        print (image_to_string(Image.open(path)))\n",
    "        print ('')\n",
    "        print ('')\n",
    "        print ('**********************************************')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
