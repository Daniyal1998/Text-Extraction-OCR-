{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Alignment (Feature Based) using OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many Computer Vision applications, we often need to identify interesting stable points in an image. \n",
    "These points are called keypoints or feature points. \n",
    "There are several keypoint detectors implemented in OpenCV ( e.g. SIFT, SURF, and ORB).\n",
    "\n",
    "ORB stands for Oriented FAST and Rotated BRIEF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A feature point detector has two parts :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locator: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This identifies points on the image that are stable under image transformations like translation, scale and rotation.\n",
    "The locator finds the x, y coordinates of such points. The locator used by the ORB detector is called FAST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The locator in the above step only tells us where the interesting points are.\n",
    "The second part of the feature detector is the descriptor which encodes the appearance of the point \n",
    "Descriptor can tell one feature point from the other. \n",
    "Ideally, the same physical point in two images should have the same descriptor. \n",
    "ORB uses a modified version of the feature descriptor called BRISK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Homography that relates the two images can be calculated only if we know corresponding features in the two images. \n",
    "So a matching algorithm is used to find which features in one image match features in the other image. \n",
    "The descriptor of every feature in one image is compared to the descriptor of every feature in the second image to find good matches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homography:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two images of a scene are related by a homography under two conditions.\n",
    "\n",
    "1. The two images are that of a plane (e.g. sheet of paper, credit card etc.).\n",
    "2. The two images were acquired by rotating the camera about its optical axis. We take such images while generating panoramas.\n",
    "\n",
    "A homography is nothing but a 3Ã—3 matrix ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](homography.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let (x1,y1) be a point in the first image and (x2,y2) be the coordinates of the same physical point in the second image.\n",
    "Then, the Homography H relates them in the following way :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](homography2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we knew the homography, we could apply it to all the pixels of one image to obtain a warped image that is aligned with the second image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignImages(im1, im2):\n",
    "    \n",
    "    MAX_FEATURES = 500\n",
    "    GOOD_MATCH_PERCENT = 0.15\n",
    " \n",
    "    # Convert images to grayscale\n",
    "    im1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "    im2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "   \n",
    "    # Detect ORB features and compute descriptors.\n",
    "    orb = cv2.ORB_create(MAX_FEATURES)\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(im1Gray, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(im2Gray, None)\n",
    "   \n",
    "    # Match features.\n",
    "    matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)\n",
    "    matches = matcher.match(descriptors1, descriptors2, None)\n",
    "   \n",
    "    # Sort matches by score\n",
    "    matches.sort(key=lambda x: x.distance, reverse=False)\n",
    " \n",
    "    # Remove not so good matches\n",
    "    numGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n",
    "    matches = matches[:numGoodMatches]\n",
    " \n",
    "    # Draw top matches\n",
    "    imMatches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)\n",
    "    #cv2.imwrite(\"matches.jpg\", imMatches)\n",
    "   \n",
    "    # Extract location of good matches\n",
    "    points1 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    "    points2 = np.zeros((len(matches), 2), dtype=np.float32)\n",
    " \n",
    "    for i, match in enumerate(matches):\n",
    "        points1[i, :] = keypoints1[match.queryIdx].pt\n",
    "        points2[i, :] = keypoints2[match.trainIdx].pt\n",
    "   \n",
    "    # Find homography\n",
    "    h, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n",
    " \n",
    "    # Use homography\n",
    "    height, width, channels = im2.shape\n",
    "    im1Reg = cv2.warpPerspective(im1, h, (width, height))\n",
    "   \n",
    "    return im1Reg, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for Feature Based Image Alignment :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Images :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first read the reference image (or the template image) and the image we want to align to this template in Lines 56-65 in the Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then detect ORB features in the two images. Although we need only 4 features to compute the homography, typically hundreds of features are detected in the two images. We control the number of features using the parameter MAX_FEATURES in the Python Code.Lines 16-19 in the Python code detect features and compute the descriptors using detectAndCompute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lines 21-34 in Python we find the matching features in the two images, sort them by goodness of match and keep only a small percentage of original matches. We finally display the good matches on the images and write the file to disk for visual inspection. We use the hamming distance as a measure of similarity between two feature descriptors. The matched features are shown in the figure below by drawing a line connecting them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Homography:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A homography can be computed when we have 4 or more corresponding points in two images. Automatic feature matching explained in the previous section does not always produce 100% accurate matches. It is not uncommon for 20-30% of the matches to be incorrect. Fortunately, the findHomography method utilizes a robust estimation technique called Random Sample Consensus (RANSAC) which produces the right result even in the presence of large number of bad matches. Lines 36-45 in Python accomplish this in code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warping image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once an accurate homography has been calculated, the transformation can be applied to all pixels in one image to map it to the other image. This is done using the warpPerspective function in OpenCV. This is accomplished in Line 49 in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps for Feature Based Image Alignment :-\n",
    "\n",
    "# Read Images :\n",
    "\n",
    "We first read the reference image (or the template image) and the image we want to align to this template in Lines 56-65 in the Python code.\n",
    "\n",
    "# Detect Features:\n",
    "\n",
    "We then detect ORB features in the two images. Although we need only 4 features to compute the homography, typically hundreds of features are detected in the two images. We control the number of features using the parameter MAX_FEATURES in the Python Code.Lines 16-19 in the Python code detect features and compute the descriptors using detectAndCompute.\n",
    "\n",
    "# Match Features:\n",
    "\n",
    " Lines 21-34 in Python we find the matching features in the two images, sort them by goodness of match and keep only a small percentage of original matches. We finally display the good matches on the images and write the file to disk for visual inspection. We use the hamming distance as a measure of similarity between two feature descriptors. The matched features are shown in the figure below by drawing a line connecting them.\n",
    "\n",
    "# Calculate Homography:\n",
    "\n",
    " A homography can be computed when we have 4 or more corresponding points in two images. Automatic feature matching explained in the previous section does not always produce 100% accurate matches. It is not uncommon for 20-30% of the matches to be incorrect. Fortunately, the findHomography method utilizes a robust estimation technique called Random Sample Consensus (RANSAC) which produces the right result even in the presence of large number of bad matches. Lines 36-45 in Python accomplish this in code.\n",
    "\n",
    "# Warping image:\n",
    "\n",
    "Once an accurate homography has been calculated, the transformation can be applied to all pixels in one image to map it to the other image. This is done using the warpPerspective function in OpenCV. This is accomplished in Line 49 in Python.\n",
    "def path_for_alignment(test_image_path,reference_image_path):\n",
    "    # Read reference image\n",
    "    refFilename = reference_image_path\n",
    "    \n",
    "    #Using the inbuilt cv2 function to read the image.  \n",
    "    imReference = cv2.imread(refFilename, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    # Read image to be aligned\n",
    "    imFilename = test_image_path\n",
    "    \n",
    "    #Using the inbuilt cv2 function to read the image.  \n",
    "    im = cv2.imread(imFilename, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    # Registered image will be resotred in imReg. \n",
    "    # The estimated homography will be stored in h. \n",
    "    imReg, h = alignImages(im, imReference)\n",
    "    \n",
    "    # Write aligned image to disk. \n",
    "    outFilename = test_image_path\n",
    "    \n",
    "    #Using cv2 inbuilt function to save the output image  \n",
    "    cv2.imwrite(outFilename, imReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morphological Transformations :-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morphological transformations are some simple operations based on the image shape. It is normally performed on binary images. It needs two inputs, one is our original image, second one is called structuring element or kernel which decides the nature of operation. Two basic morphological operators are Erosion and Dilation. Then its variant forms like Opening, Closing, Gradient etc also comes into play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphological_transformation(path):\n",
    "    #Reading the Image and converting into gray_scale using the 2nd parameter of imread()\n",
    "    image = cv2.imread(path,0)\n",
    "    kernel = np.ones((1,1),np.uint8)\n",
    "\n",
    "    #Applying Dialation and Erosion to remove noise\n",
    "    image = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\n",
    "    cv2.imwrite(path,image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    image_list = glob.glob(folder+'/*.jpg')\n",
    "    #add_img = glob.glob(folder+'/*.jpeg')\n",
    "    #image_list.append(img for img in add_img)\n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__== \"__main__\":\n",
    "    images_path = load_images_from_folder(r'C:\\Users\\dell\\CitiProject\\imgfol')\n",
    "    model = load_model('C:/Users/dell/CitiProject/model/akshika.h5')\n",
    "    for path in images_path:\n",
    "        morphological_transformation(path)\n",
    "        \n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img,(64,64))\n",
    "        img = np.reshape(img,[1,64,64,3])\n",
    "        classes = model.predict_classes(img)\n",
    "        \n",
    "        image_type = classes[0][0]\n",
    "        \n",
    "        reference_path = r'C:\\Users\\dell\\CitiProject\\referencefol'\n",
    "        reference_path = reference_path + r'\\' + str(image_type) + '.jpg'\n",
    "        \n",
    "        path_for_alignment(path , reference_path)\n",
    "        \n",
    "        print (image_to_string(Image.open(path)))\n",
    "        print ('')\n",
    "        print ('')\n",
    "        print ('**********************************************')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
